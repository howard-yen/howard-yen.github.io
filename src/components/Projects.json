 {
    "UnderwaterColorization":{
        "title": "Exploring Underwater Image Enhancement",
        "url": "/colorization.pdf",
        "icon": "Scuba",
        "description": "Underwater image enhancement seeks to augment the image quality of photos taken underwater by correcting for artifacts like backscatter, light attenuation, and other sources of haze. We approach the task of correcting images taken underwater by exploring both a two-stage convolutional network based approach, as well as a large-scale pre-trained transformer based approach using VAEs.",
        "others": {

        }
    },
    "RTFM BERT":{
        "title": "Read to Fight Monsters: FiLM to BERT",
        "url": "https://github.com/howard-yen/RTFM",
        "icon": "Game",
        "description": "In previous reinforcement learning setups, the most commonly used models are typically transformers and convolution layers trained from scratch, but we are interested in using large pretrained models like BERT instead. Furthermore, we want to amend the RTFM framework (Zhong 2020) by adding different world representations such as all text and text + image. Finally, we test BERT models on this framework by training BERT-Tiny with different setups.",
        "others": {

        }
    },
    "Universal Triggers":{
        "title": "Universal Triggers",
        "url": "https://github.com/howard-yen/universal-triggers",
        "icon": "Attack",
        "description": "In this project, we analyze universal adversarial triggers on various auto-regressive generative language models using the setup and methods from (Wallace 2019). We analyzed the models GPT2, DistilGPT2, GPTNeo, and XLNet. For each model, we first generate 10 trigger tokens, and select the tokens with the lowest loss on the handwritten inputs, and then use the trigger to generate 100 output on all 4 models. Finally, we manually analyze  all generated outputs for racist and/or offensive text.",
        "others": {

        }
    },
    "TigerResearch":{
        "title": "TigerResearch",
        "url": "https://tigerresearch.herokuapp.com/",
        "icon": "Matching",
        "description": "This is a project for the class COS333 and was created in coperation with Princeton's Office of Undergraduate Research(OUR). We created a web app that allow undergrads to browse and search all the research mentors available in ReMatch, a research pairing program hosted by OUR. It also lets the mentors to browse and search the undergrads who have signed up for the program. Lastly, it also allow the program administrators to automatically pair mentors and mentees using a matching algoithm given each mentee's prefernces and manually make edits.",
        "others": {

        }
    },
    "Font Generation":{
        "title": "Font Generation",
        "url": "https://github.com/howard-yen/cos429-finalproject",
        "icon": "Fonts",
        "description": "For our final project in Computer Vision, we trained a Deep Convolutional Neural Network consisted of an autoencoder and a discriminator that takes in one letter of a certain style and outputs a full alphabet with a consistent, matching style. We trained the model on fonts from Google Fonts, where we converted ttf to png, and the model was able to output all capitalized letters in consistent and matching style when only inputting an image of the letter R.",
        "others": {

        }
    },
    "Operating System":{
        "title": "Operating System",
        "url": "https://www.cs.princeton.edu/courses/archive/fall20/cos318/projects.html",
        "icon": "Terminal",
        "description": "A series of project in the class Operating Systems, where we progressively built up our own opearting system. The OS include: a bootloader, a preemptive scheduler using synchronization primitives, inter-processing communcation using mailboxes, process management, virtual memory with Second Chance paging policy and also Not Recently Used page replacement algorithm, and a file system",
        "others":{
            "Note": "Code is not public due to class policy but available upon request"
        }
    },
    "Parallel Sequences":{
        "title": "Parallel Sequences",
        "url": "https://www.cs.princeton.edu/courses/archive/fall20/cos326/ass/a7.php",
        "icon": "Parallel",
        "description": "In this Functional Programming assignment, we applied functional map-reduce abstraction to search engine indexing by building an inverted index as well as geographic queries using cumulative sums. We also performed work and span analysis on the resulting program to assess the use of parallel schemes in our program.",
        "others":{
            "Note": "Code is not public due to class policy but available upon request"
        }
    },
    "spotifind":{
        "title": "spotifind",
        "url": "https://github.com/mathewjhan/spotifind",
        "icon": "Spotifind",
        "description": "Focused on connectivity and sparking new friendships through music, this IvyHacks project utilized machine learning algorithm and the Spotify API to recommend users music based on their taste as well as the people around them. We built the frontend using React JS and the backend using Python, and integrated the two using Flask. We fetched the user's most popular artist and songs using the Spotify API, the user's geolocation data and the nearby users using the web browser's geolocation API, and then recommend to the user the music that are the most similar to their taste using K Nearest Neighbors algorithm.",
        "others":{

        }
    },
    "mangadex-download":{
        "title": "mangadex-download",
        "url": "https://howard-yen.github.io/mangadex-download/",
        "icon": "Download",
        "description": "A project born out of my love for reading and manga. I utilized the requests, lxml, PIL, and smtplib Python libraries to scrape the website mangadex.org for images.\nFunctionality include using browser cookies bypass login requirements on the website to search for specific titles, downloading a selection of the chapters as pdf files by first scraping individual images of each chapter, and emailing the files to the user.",
        "others": {

        }
    },
    "pdfsummary":{
        "title": "pdfsummary",
        "url": "https://github.com/archen2019/pdfsummary",
        "icon": "Agenda",
        "description": "Motivated by the struggles of reading hundreds of papers for classes, my partner and I developed this Python script aimed to makes students' lives easier and submitted it as a project for HackPrinceton 2019.\nWe first used computer vision to read in every page of a pdf file as images and extracted the content as texts. Then we used a summarization algorithm to find the key words in the content and produce a summary for the entire file.",
        "others": {

        }
    },
    "terminal-ai":{
        "title": "terminal-ai",
        "url": "https://github.com/archen2019/terminal-ai",
        "icon": "Chess",
        "description": "Terminal AI is an online game where two algorithms face off against each other in a tower-defense-like game where each player can deploy a variety of attacking units and defensive towers. In a team of three, we coded offensive and defensive algorithms in Python to compete against other algorithms.",
        "others": {
            "Award": "Second Place at Citadel Terminal Live 2020 Princeton vs. Penn"
        }
    },
    "uptime analysis":{
        "title": "uptime analysis",
        "url": "https://docs.google.com/presentation/d/1sCf-PyHFVULJR9gPqhILHL1Lkp5Oxbpz4o2ilrraWkg/edit?usp=sharing",
        "icon": "Clock",
        "description": "Final project for the class WRI150: Your Life in Numbers, where I analyzed the relationship between my computer usage and typing speed.\nThroughout the 12 weeks of the freshmen spring semester, I recorded the amount of time I spend on my laptop everyday by writing a shell script on my Arch Linux OS that recorded the system uptime everytime I poweroff or reboot my laptop. I also recorded my typing speed everyday by doing 3 typing tests on typeracer.com. Finally, I conducted statistical analysis in R to find correlations between the two.",
        "others":{

        }
    },
    "Seam Carving":{
        "title": "Seam Carving",
        "url": "https://www.cs.princeton.edu/courses/archive/fall20/cos226/assignments/seam/specification.php",
        "icon": "Photo",
        "description": "An assignment for the class COS226: Algorithms and Data Structures. The goal was to find the best path in an image to remove and remove them dynamically.\nThe best horizontal/vertical path in a photo is one that has the least energy changes, where the weight of the edges are the square differences between the rgb values of each pixel. I initially implemented Dijkstra's algorithm in Java to find such paths and then optimized it with topological sort.",
        "others":{
            "Note": "Code is not public due to class policy but available upon request"
        }
    },
    "Burrows-Wheeler":{
        "title": "Burrows-Wheeler",
        "url": "https://www.cs.princeton.edu/courses/archive/fall20/cos226/assignments/burrows/specification.php",
        "icon": "Compressed",
        "description": "An assignment for the class COS226: Algorithms and Data Structures. This is an implementation of the Burrows-Wheeler data compression algorithm in Java.\nThis consisted of Burrows-Wheeler Transform and move-to-front encoding. To do this efficiently, I used most significant digit sort to construct a circular array which contains optimized references to the original string that is to be compressed.",
        "others":{
            "Note": "Code is not public due to class policy but available upon request"
        }
    },
    "Unix Shell":{
        "title": "shell",
        "url": "https://www.cs.princeton.edu/courses/archive/spr19/cos217/asgts/07shell/",
        "icon": "Terminal",
        "description": "An assignment for the class COS217: Introduction to Programming Systems. My partner and I designed a basic Unix Shell in C.\nThis contains a lexical analyzer and a syntactic analyzer which parse the input command and determine if they are valid, and also handling for internal and external commands, input/output redirections, and signals.",
        "others":{
            "Note": "Code is not public due to class policy but available upon request"
        }
    },
    "Twitter: Bull or Bear":{
        "title": "Twitter: Bull or Bear",
        "url":"https://docs.google.com/document/d/1ksdLPEeKOQaKms8buKNl6TGrtTmtWtJBzh3CUqxLn_M/edit?usp=sharing",
        "icon": "Stocks",
        "description": "An assignment for the class WRI150: Your Life in Numbers. In this project, I explored the relationship between a company's Twitter account vs. various financial metrics.\nFirst, I collected companies' Twitter follower and tweet history by using Python to scrap https://www.trackalytics.com, then collected companies quarters and stock data from NASDAQ. Then, I used linear regression to find trends between the statistics.",
        "others":{

        }
    },
    "mergepdf":{
        "title": "mergepdf",
        "url": "https://github.com/howard-yen/mergepdf",
        "icon": "Book",
        "description": "A simple Python script that merges all the pdf files in a given directory into one pdf file. This was originally used as a study tool to combine class notes together so that it's easier to reference materials. I also use this as a way to concatenate lecture slides together for ease of seraching for terms on open notes exams.",
        "others": {

        }
    }
}
